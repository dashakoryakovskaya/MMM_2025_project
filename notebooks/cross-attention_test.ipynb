{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb0a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947d0ac",
   "metadata": {},
   "source": [
    "#### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "    df['day'] = df['activation_date'].dt.day\n",
    "    df['month'] = df[\"activation_date\"].dt.month\n",
    "    df['year'] = df[\"activation_date\"].dt.year\n",
    "    df['weekday'] = df['activation_date'].dt.weekday\n",
    "    df[\"dayofyear\"] = df['activation_date'].dt.dayofyear\n",
    "    df.drop(columns=['activation_date', 'item_id'], inplace=True)\n",
    "    df['param_1'] = df['param_1'].fillna('')\n",
    "    df['param_2'] = df['param_2'].fillna('')\n",
    "    df['param_3'] = df['param_3'].fillna('')\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    return df\n",
    "\n",
    "item_id = test.item_id\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edf826",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0bf2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.gelu(x)  # Более плавная активация\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_2(x)\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + self.dropout(residual))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(1)].detach()  # Отключаем градиенты\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout=0.1, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.self_attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.feed_forward = PositionWiseFeedForward(input_dim, input_dim, dropout=dropout)\n",
    "        self.add_norm_after_attention = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.add_norm_after_ff = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.positional_encoding = PositionalEncoding(input_dim) if positional_encoding else None\n",
    "\n",
    "    def forward(self, key, value, query):\n",
    "        if self.positional_encoding:\n",
    "            key = self.positional_encoding(key)\n",
    "            value = self.positional_encoding(value)\n",
    "            query = self.positional_encoding(query)\n",
    "\n",
    "        attn_output, _ = self.self_attention(query, key, value, need_weights=False)\n",
    "\n",
    "        x = self.add_norm_after_attention(attn_output, query)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.add_norm_after_ff(ff_output, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1c44f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, first_dim=768, second_dim=1024, hidden_dim=512, num_transformer_heads=2, positional_encoding=True, dropout=0, mode='mean', device=\"cuda\",  tr_layer_number=1, out_features=128):\n",
    "        super(MultiModalTransformer, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Проекционные слои\n",
    "\n",
    "        self.first_proj = nn.Sequential(\n",
    "            nn.Conv1d(first_dim, hidden_dim, 1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.second_proj = nn.Sequential(\n",
    "            nn.Conv1d(second_dim, hidden_dim, 1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Механизмы внимания\n",
    "        self.first_to_second_attn = nn.ModuleList([TransformerEncoderLayer(input_dim=hidden_dim, num_heads=num_transformer_heads, positional_encoding=positional_encoding, dropout=dropout) for i in range(tr_layer_number)\n",
    "                ])\n",
    "        self.second_to_first_attn = nn.ModuleList([TransformerEncoderLayer(input_dim=hidden_dim, num_heads=num_transformer_heads, positional_encoding=positional_encoding, dropout=dropout) for i in range(tr_layer_number)\n",
    "                ])\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, out_features) if self.mode == 'mean' else nn.Linear(hidden_dim*4, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, first_features, second_features):\n",
    "        # Преобразование размерностей\n",
    "        first_features = first_features.float()\n",
    "        second_features = second_features.float()\n",
    "\n",
    "        first_features = self.first_proj(first_features.permute(0,2,1)).permute(0,2,1)\n",
    "        second_features = self.second_proj(second_features.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "        # Адаптивная пуллинг до минимальной длины\n",
    "        min_seq_len = min(first_features.size(1), second_features.size(1))\n",
    "        first_features = F.adaptive_avg_pool1d(first_features.permute(0,2,1), min_seq_len).permute(0,2,1)\n",
    "        second_features = F.adaptive_avg_pool1d(second_features.permute(0,2,1), min_seq_len).permute(0,2,1)\n",
    "\n",
    "        # Трансформерные блоки\n",
    "        for i in range(len(self.first_to_second_attn)):\n",
    "            attn_first = self.first_to_second_attn[i](second_features, first_features, first_features)\n",
    "            attn_second = self.second_to_first_attn[i](first_features, second_features, second_features)\n",
    "            first_features += attn_first\n",
    "            second_features += attn_second\n",
    "\n",
    "        # Статистики\n",
    "        std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
    "        std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
    "\n",
    "        # Классификация\n",
    "        if self.mode == 'mean':\n",
    "            return self.out(torch.cat([mean_first, mean_first], dim=1))\n",
    "        else:\n",
    "            std_first = torch.nan_to_num(std_first, nan=0.0)\n",
    "            std_second = torch.nan_to_num(std_second, nan=0.0)\n",
    "            return self.out(torch.cat([mean_first, std_first, mean_second, std_second], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a46152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jina_list = sorted(os.listdir('../data/jina'), key= lambda x: int(x.replace(\"jina_test_\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53160ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b2dbfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = MultiModalTransformer(first_dim=1024, second_dim=768)\n",
    "checkpoint = torch.load(\"models/MultiModalTransformer_5_0.82_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c789479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/508438 [00:02<337:20:13,  2.39s/it]/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/3554938142.py:56: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
      "/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/3554938142.py:57: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
      "100%|██████████| 508438/508438 [13:00<00:00, 651.50it/s] \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 768)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "    y_pred.append(float(model(text_embedding, image_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d025757",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.clip(y_pred, 0, 1)\n",
    "pd.DataFrame({'item_id': item_id, 'deal_probability': result}).to_csv(\"../results/cross-attention_mean_.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78968e8f",
   "metadata": {},
   "source": [
    "Результат: 0.25820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02fba5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = MultiModalTransformer(first_dim=1024, second_dim=768, mode='not_mean')\n",
    "checkpoint = torch.load(\"models/MultiModalTransformer_5_0.81_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c909d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/508438 [00:01<282:14:53,  2.00s/it]/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/3554938142.py:56: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
      "/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/3554938142.py:57: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
      "100%|██████████| 508438/508438 [13:35<00:00, 623.69it/s] \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 768)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "    y_pred.append(float(model(text_embedding, image_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b079f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.clip(y_pred, 0, 1)\n",
    "pd.DataFrame({'item_id': item_id, 'deal_probability': result}).to_csv(\"../results/cross-attention_not_mean_.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973391d",
   "metadata": {},
   "source": [
    "Результат: 0.36499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76af25f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dashakoryak/Desktop/MMM проект/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv, RGCNConv, TransformerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d077646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFusionLayerAtt(nn.Module):\n",
    "    def __init__(self, hidden_dim, heads=2):\n",
    "        super().__init__()\n",
    "        # Проекционные слои для признаков\n",
    "        self.proj_audio = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.proj_text = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Графовые слои\n",
    "        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=heads)\n",
    "        self.gat2 = GATConv(hidden_dim*heads, hidden_dim)\n",
    "\n",
    "        self.attention_fusion = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Финальная проекция\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def build_complete_graph(self, num_nodes):\n",
    "        # Создаем полный граф (каждый узел соединен со всеми)\n",
    "        edge_index = []\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if i != j:\n",
    "                    edge_index.append([i, j])\n",
    "        return torch.tensor(edge_index).t().contiguous()\n",
    "\n",
    "    def forward(self, first_stats, second_stats):\n",
    "        \"\"\"\n",
    "        first_stats: [batch_size, hidden_dim]\n",
    "        second_stats: [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size = first_stats.size(0)\n",
    "\n",
    "        # Проекция признаков\n",
    "        x_first = F.relu(self.proj_audio(first_stats))  # [batch_size, hidden_dim]\n",
    "        x_second = F.relu(self.proj_text(second_stats))    # [batch_size, hidden_dim]\n",
    "\n",
    "        # Объединение узлов (1 и 2 попеременно)\n",
    "        nodes = torch.stack([x_first, x_second], dim=1)  # [batch_size, 2, hidden_dim]\n",
    "        nodes = nodes.view(-1, nodes.size(-1))        # [batch_size*2, hidden_dim]\n",
    "\n",
    "        # Построение графа (полный граф для каждого элемента батча)\n",
    "        edge_index = self.build_complete_graph(2)  # Граф для одной пары 1-2\n",
    "        edge_index = edge_index.to(first_stats.device)\n",
    "\n",
    "        # Применение GAT\n",
    "        x = F.relu(self.gat1(nodes, edge_index))\n",
    "        x = self.gat2(x, edge_index)\n",
    "\n",
    "        # Разделяем обратно аудио и текст\n",
    "        x = x.view(batch_size, 2, -1)  # [batch_size, 2, hidden_dim]\n",
    "\n",
    "        # Усреднение по модальностям\n",
    "        # fused = torch.mean(x, dim=1)   # [batch_size, hidden_dim]\n",
    "\n",
    "        weights = F.softmax(self.attention_fusion(x), dim=1)\n",
    "        fused = torch.sum(weights * x, dim=1)  # [batch_size, hidden_dim]\n",
    "\n",
    "        return self.fc(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfd6a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, first_dim=768, second_dim=1024, hidden_dim=512, num_transformer_heads=2, positional_encoding=True, dropout=0, mode='mean', device=\"cuda\",  tr_layer_number=1, out_features=128, num_heads=2):\n",
    "        super(MultiModalTransformer, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Проекционные слои\n",
    "\n",
    "        self.first_proj = nn.Sequential(\n",
    "            nn.Conv1d(first_dim, hidden_dim, 1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.second_proj = nn.Sequential(\n",
    "            nn.Conv1d(second_dim, hidden_dim, 1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Механизмы внимания\n",
    "        self.first_to_second_attn = nn.ModuleList([TransformerEncoderLayer(input_dim=hidden_dim, num_heads=num_transformer_heads, positional_encoding=positional_encoding, dropout=dropout) for i in range(tr_layer_number)\n",
    "                ])\n",
    "        self.second_to_first_attn = nn.ModuleList([TransformerEncoderLayer(input_dim=hidden_dim, num_heads=num_transformer_heads, positional_encoding=positional_encoding, dropout=dropout) for i in range(tr_layer_number)\n",
    "                ])\n",
    "        \n",
    "        # Графовое слияние\n",
    "        if self.mode == 'mean':\n",
    "            self.graph_fusion = GraphFusionLayerAtt(hidden_dim, heads=num_heads)\n",
    "        else:\n",
    "            self.graph_fusion = GraphFusionLayerAtt(hidden_dim*2, heads=num_heads)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_features) if self.mode == 'mean' else nn.Linear(hidden_dim*2, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, first_features, second_features):\n",
    "        # Преобразование размерностей\n",
    "        first_features = first_features.float()\n",
    "        second_features = second_features.float()\n",
    "\n",
    "        first_features = self.first_proj(first_features.permute(0,2,1)).permute(0,2,1)\n",
    "        second_features = self.second_proj(second_features.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "        # Адаптивная пуллинг до минимальной длины\n",
    "        min_seq_len = min(first_features.size(1), second_features.size(1))\n",
    "        first_features = F.adaptive_avg_pool1d(first_features.permute(0,2,1), min_seq_len).permute(0,2,1)\n",
    "        second_features = F.adaptive_avg_pool1d(second_features.permute(0,2,1), min_seq_len).permute(0,2,1)\n",
    "\n",
    "        # Трансформерные блоки\n",
    "        for i in range(len(self.first_to_second_attn)):\n",
    "            attn_first = self.first_to_second_attn[i](second_features, first_features, first_features)\n",
    "            attn_second = self.second_to_first_attn[i](first_features, second_features, second_features)\n",
    "            first_features += attn_first\n",
    "            second_features += attn_second\n",
    "\n",
    "        # Статистики\n",
    "        std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
    "        std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
    "\n",
    "        # Графовое слияние статистик\n",
    "        if self.mode == 'mean':\n",
    "            h_ta = self.graph_fusion(mean_first, mean_second)\n",
    "        else:\n",
    "            std_first = torch.nan_to_num(std_first, nan=0.0)\n",
    "            std_second = torch.nan_to_num(std_second, nan=0.0)\n",
    "            h_ta = self.graph_fusion(torch.cat([mean_first, std_first], dim=1), torch.cat([mean_second, std_second], dim=1))\n",
    "\n",
    "        # Классификация\n",
    "        return self.classifier(h_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eea661c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = MultiModalTransformer(first_dim=1024, second_dim=768)\n",
    "checkpoint = torch.load(\"models/MultiModalTransformer_3_0.81_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcb0aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/508438 [00:02<327:14:00,  2.32s/it]/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/1038632279.py:62: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
      "/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/1038632279.py:63: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
      "100%|██████████| 508438/508438 [19:49<00:00, 427.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 768)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "    y_pred.append(float(model(text_embedding, image_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c0b3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.clip(y_pred, 0, 1)\n",
    "pd.DataFrame({'item_id': item_id, 'deal_probability': result}).to_csv(\"../results/cross-attention_mean_graph_fusion.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518b668",
   "metadata": {},
   "source": [
    "Результат: 0.24969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0d54166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = MultiModalTransformer(first_dim=1024, second_dim=768, mode='not_mean')\n",
    "checkpoint = torch.load(\"models/MultiModalTransformer_6_0.82_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08a4e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/508438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/508438 [00:02<287:33:15,  2.04s/it]/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/1038632279.py:62: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_first, mean_first = torch.std_mean(attn_first, dim=1)\n",
      "/var/folders/w3/2phmjbtn2_9__fxnss244s680000gn/T/ipykernel_25293/1038632279.py:63: UserWarning: std_mean(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std_second, mean_second = torch.std_mean(attn_second, dim=1)\n",
      "100%|██████████| 508438/508438 [27:20<00:00, 310.01it/s]  \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 768)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "    y_pred.append(float(model(text_embedding, image_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5820829",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.clip(y_pred, 0, 1)\n",
    "pd.DataFrame({'item_id': item_id, 'deal_probability': result}).to_csv(\"../results/cross-attention_not_mean_graph_fusion.csv\", index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
