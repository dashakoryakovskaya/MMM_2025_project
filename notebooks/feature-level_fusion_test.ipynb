{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947d0ac",
   "metadata": {},
   "source": [
    "#### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "    df['day'] = df['activation_date'].dt.day\n",
    "    df['month'] = df[\"activation_date\"].dt.month\n",
    "    df['year'] = df[\"activation_date\"].dt.year\n",
    "    df['weekday'] = df['activation_date'].dt.weekday\n",
    "    df[\"dayofyear\"] = df['activation_date'].dt.dayofyear\n",
    "    df.drop(columns=['activation_date', 'item_id'], inplace=True)\n",
    "    df['param_1'] = df['param_1'].fillna('')\n",
    "    df['param_2'] = df['param_2'].fillna('')\n",
    "    df['param_3'] = df['param_3'].fillna('')\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    return df\n",
    "\n",
    "item_id = test.item_id\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edf826",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0bf2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim = 1024, hidden_dim=128, num_heads = 4, num_layers = 8, dropout = 0.1):\n",
    "        super(TransformerModelWithAttention, self).__init__()\n",
    "        self.in_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 10000, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = hidden_dim, nhead = num_heads, dim_feedforward = hidden_dim, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.in_layer(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "        encoder_output = self.transformer_encoder(x)\n",
    "        x = encoder_output.mean(dim = 1)\n",
    "        return torch.clamp(self.fc_out(x), 0.0, 1.0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a46152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jina_list = sorted(os.listdir('../data/jina'), key= lambda x: int(x.replace(\"jina_test_\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53160ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e6bf33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2)\n",
    "checkpoint = torch.load(\"models/TransformerModelWithAttention_1_0.89_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df97400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508438/508438 [10:21<00:00, 818.04it/s] \n"
     ]
    }
   ],
   "source": [
    "user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    tabular = torch.tensor([row[\"item_seq_number\"], row[\"day\"], row[\"month\"], row[\"year\"], row[\"weekday\"], row[\"dayofyear\"], user_type_dict[row[\"user_type\"]], 0.0 if row[\"price\"] is None else row[\"price\"]])\n",
    "    tabular = tabular.unsqueeze(0).unsqueeze(2).expand(-1, -1, 1024)\n",
    "    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "    emb_concat = torch.concat((tabular, text_embedding), 1)\n",
    "    y_pred.append(float(model(emb_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cd0befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'item_id': item_id, 'deal_probability': y_pred}).to_csv(\"../results/feature-level-Transformer.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9e13f",
   "metadata": {},
   "source": [
    "Результат: 0.30322"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e05fb",
   "metadata": {},
   "source": [
    "### Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e1a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import silu\n",
    "from torch.nn.functional import softplus\n",
    "from einops import rearrange, repeat, einsum\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:        \n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps) * self.weight\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, num_layers, d_input, d_model, d_state=16, d_discr=None, ker_size=4):\n",
    "        super().__init__()\n",
    "        mamba_par = {\n",
    "            'd_input' : d_input,\n",
    "            'd_model' : d_model,\n",
    "            'd_state' : d_state,\n",
    "            'd_discr' : d_discr,\n",
    "            'ker_size': ker_size\n",
    "        }\n",
    "        self.layers = nn.ModuleList([nn.ModuleList([MambaBlock(**mamba_par), RMSNorm(d_input)]) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_input, 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        seq = seq.to(self.device)\n",
    "        for mamba, norm in self.layers:\n",
    "            out, cache = mamba(norm(seq), cache)\n",
    "            seq = out + seq\n",
    "        return self.fc_out(seq.mean(dim = 1))\n",
    "        \n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model, d_state=16, d_discr=None, ker_size=4):\n",
    "        super().__init__()\n",
    "        d_discr = d_discr if d_discr is not None else d_model // 16\n",
    "        self.in_proj  = nn.Linear(d_input, 2 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_input, bias=False)\n",
    "        self.s_B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_C = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_D = nn.Sequential(nn.Linear(d_model, d_discr, bias=False), nn.Linear(d_discr, d_model, bias=False),)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=ker_size,\n",
    "            padding=ker_size - 1,\n",
    "            groups=d_model,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.arange(1, d_state + 1, dtype=torch.float).repeat(d_model, 1))\n",
    "        self.D = nn.Parameter(torch.ones(d_model, dtype=torch.float))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        b, l, d = seq.shape\n",
    "        (prev_hid, prev_inp) = cache if cache is not None else (None, None)\n",
    "        a, b = self.in_proj(seq).chunk(2, dim=-1)\n",
    "        x = rearrange(a, 'b l d -> b d l')\n",
    "        x = x if prev_inp is None else torch.cat((prev_inp, x), dim=-1)\n",
    "        a = self.conv(x)[..., :l]\n",
    "        a = rearrange(a, 'b d l -> b l d')\n",
    "        a = silu(a)\n",
    "        a, hid = self.ssm(a, prev_hid=prev_hid) \n",
    "        b = silu(b)\n",
    "        out = a * b\n",
    "        out =  self.out_proj(out)\n",
    "        if cache:\n",
    "            cache = (hid.squeeze(), x[..., 1:])   \n",
    "        return out, cache\n",
    "    \n",
    "    def ssm(self, seq, prev_hid):\n",
    "        A = -self.A\n",
    "        D = +self.D\n",
    "        B = self.s_B(seq)\n",
    "        C = self.s_C(seq)\n",
    "        s = softplus(D + self.s_D(seq))\n",
    "        A_bar = einsum(torch.exp(A), s, 'd s,   b l d -> b l d s')\n",
    "        B_bar = einsum(          B,  s, 'b l s, b l d -> b l d s')\n",
    "        X_bar = einsum(B_bar, seq, 'b l d s, b l d -> b l d s')\n",
    "        hid = self._hid_states(A_bar, X_bar, prev_hid=prev_hid)\n",
    "        out = einsum(hid, C, 'b l d s, b l s -> b l d')\n",
    "        out = out + D * seq\n",
    "        return out, hid\n",
    "    \n",
    "    def _hid_states(self, A, X, prev_hid=None):\n",
    "        b, l, d, s = A.shape\n",
    "        A = rearrange(A, 'b l d s -> l b d s')\n",
    "        X = rearrange(X, 'b l d s -> l b d s')\n",
    "        if prev_hid is not None:\n",
    "            return rearrange(A * prev_hid + X, 'l b d s -> b l d s')\n",
    "        h = torch.zeros(b, d, s, device=self.device)\n",
    "        return torch.stack([h := A_t * h + X_t for A_t, X_t in zip(A, X)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5190718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mamba(num_layers=2, d_input=1024, d_model=128).to(device)\n",
    "checkpoint = torch.load(\"models/Mamba_11_4.28_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0411890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508438/508438 [1:18:50<00:00, 107.48it/s] \n"
     ]
    }
   ],
   "source": [
    "user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    tabular = torch.tensor([row[\"item_seq_number\"], row[\"day\"], row[\"month\"], row[\"year\"], row[\"weekday\"], row[\"dayofyear\"], user_type_dict[row[\"user_type\"]], 0.0 if row[\"price\"] is None else row[\"price\"]])\n",
    "    tabular = tabular.unsqueeze(0).unsqueeze(2).expand(-1, -1, 1024)\n",
    "    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "    emb_concat = torch.concat((tabular, text_embedding), 1)\n",
    "    y_pred.append(float(model(emb_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b4f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'item_id': item_id, 'deal_probability': np.clip(y_pred, 0, 1)}).to_csv(\"../results/feature-level-Mamba.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712dfcb",
   "metadata": {},
   "source": [
    "Результат: 0.45980"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36178be4",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facc026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 1024, hidden_size = 64, num_layers = 2, dropout = 0.1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.lstm.bidirectional:\n",
    "            h0, c0 = torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        else:\n",
    "            h0, c0 = torch.zeros(self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        if self.lstm.bidirectional:\n",
    "            out = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20f75f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM().to(device)\n",
    "checkpoint = torch.load(\"models/LSTM_1_0.89_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ed4ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508438/508438 [1:04:46<00:00, 130.82it/s]\n"
     ]
    }
   ],
   "source": [
    "user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    tabular = torch.tensor([row[\"item_seq_number\"], row[\"day\"], row[\"month\"], row[\"year\"], row[\"weekday\"], row[\"dayofyear\"], user_type_dict[row[\"user_type\"]], 0.0 if row[\"price\"] is None else row[\"price\"]])\n",
    "    tabular = tabular.unsqueeze(0).unsqueeze(2).expand(-1, -1, 1024)\n",
    "    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "    emb_concat = torch.concat((tabular, text_embedding), 1)\n",
    "    y_pred.append(float(model(emb_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94efd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'item_id': item_id, 'deal_probability': np.clip(y_pred, 0, 1)}).to_csv(\"../results/feature-level-LSTM.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5caf7b8",
   "metadata": {},
   "source": [
    "Результат: 0.27722"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
