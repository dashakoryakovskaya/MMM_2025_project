{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fb0a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947d0ac",
   "metadata": {},
   "source": [
    "#### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f9a40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dddacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "    df['day'] = df['activation_date'].dt.day\n",
    "    df['month'] = df[\"activation_date\"].dt.month\n",
    "    df['year'] = df[\"activation_date\"].dt.year\n",
    "    df['weekday'] = df['activation_date'].dt.weekday\n",
    "    df[\"dayofyear\"] = df['activation_date'].dt.dayofyear\n",
    "    df.drop(columns=['activation_date', 'item_id'], inplace=True)\n",
    "    df['param_1'] = df['param_1'].fillna('')\n",
    "    df['param_2'] = df['param_2'].fillna('')\n",
    "    df['param_3'] = df['param_3'].fillna('')\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    return df\n",
    "\n",
    "item_id = test.item_id\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edf826",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0bf2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.gelu(x)  # Более плавная активация\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_2(x)\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + self.dropout(residual))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(1)].detach()  # Отключаем градиенты\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout=0.1, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.self_attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.feed_forward = PositionWiseFeedForward(input_dim, input_dim, dropout=dropout)\n",
    "        self.add_norm_after_attention = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.add_norm_after_ff = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.positional_encoding = PositionalEncoding(input_dim) if positional_encoding else None\n",
    "\n",
    "    def forward(self, key, value, query):\n",
    "        if self.positional_encoding:\n",
    "            key = self.positional_encoding(key)\n",
    "            value = self.positional_encoding(value)\n",
    "            query = self.positional_encoding(query)\n",
    "\n",
    "        attn_output, _ = self.self_attention(query, key, value, need_weights=False)\n",
    "\n",
    "        x = self.add_norm_after_attention(attn_output, query)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.add_norm_after_ff(ff_output, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1c44f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim = 1024, hidden_dim=128, num_heads = 4, num_layers = 8, dropout = 0.1, positional_encoding=True):\n",
    "        super(TransformerModelWithAttention, self).__init__()\n",
    "        self.in_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.transformer_encoder = nn.ModuleList([TransformerEncoderLayer(input_dim=hidden_dim, num_heads=num_heads, positional_encoding=positional_encoding, dropout=dropout) for i in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.in_layer(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.positional_encoding(x)\n",
    "        for i in range(len(self.transformer_encoder)):\n",
    "            x = x + self.transformer_encoder[i](x, x, x)\n",
    "        x = x.mean(dim = 1)\n",
    "        return self.fc_out(x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8a46152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jina_list = sorted(os.listdir('../data/jina'), key= lambda x: int(x.replace(\"jina_test_\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53160ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e6bf33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2)\n",
    "checkpoint = torch.load(\"models/TransformerModelWithAttention_6_0.84_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508438/508438 [17:58<00:00, 471.54it/s]  \n"
     ]
    }
   ],
   "source": [
    "user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    tabular = torch.tensor([row[\"item_seq_number\"], row[\"day\"], row[\"month\"], row[\"year\"], row[\"weekday\"], row[\"dayofyear\"], user_type_dict[row[\"user_type\"]], 0.0 if row[\"price\"] is None else row[\"price\"]])\n",
    "    tabular = tabular.unsqueeze(0).unsqueeze(2).expand(-1, -1, 1024)\n",
    "    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "    tabular = F.normalize(tabular, dim=1, eps=1e-6)\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 1024)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "        image_embedding = F.pad(image_embedding, (0, 1024-768), \"constant\", 0)\n",
    "    emb_concat = torch.concat((tabular.to(device), text_embedding.to(device), image_embedding.to(device)), dim=1)\n",
    "    y_pred.append(float(model(emb_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0cd0befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = np.clip(y_pred, 0, 1)\n",
    "pd.DataFrame({'item_id': item_id, 'deal_probability': result_image}).to_csv(\"../results/feature-level-image-Transformer.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87b476",
   "metadata": {},
   "source": [
    "Результат: 0.31286"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36178be4",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "facc026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 1024, hidden_size = 64, num_layers = 2, dropout = 0.1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.lstm.bidirectional:\n",
    "            h0, c0 = torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        else:\n",
    "            h0, c0 = torch.zeros(self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        if self.lstm.bidirectional:\n",
    "            out = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20f75f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM().to(device)\n",
    "checkpoint = torch.load(\"models/LSTM_1_0.89_checkpoint.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7ed4ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508438/508438 [1:19:11<00:00, 107.01it/s] \n"
     ]
    }
   ],
   "source": [
    "user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "y_pred = []\n",
    "jina_list_ind = -1\n",
    "len_test = test.shape[0]\n",
    "x = 200\n",
    "for i, row in tqdm(test.iterrows(), total=len_test):\n",
    "    # text\n",
    "    if i % 10000 == 0:\n",
    "        jina_list_ind += 1\n",
    "        jina_name = jina_list[jina_list_ind]\n",
    "        with open(\"../data/jina/\" + jina_name, \"rb\") as f:  \n",
    "            jina_emb = CPU_Unpickler(f).load()\n",
    "    # image\n",
    "    if i == 32001:\n",
    "        x = 100\n",
    "    if i <= 36600 and i % 200 == 0:\n",
    "        with open(\"../data/vit/vit_test_jpg_\" + str(i-1+200), \"rb\") as f: \n",
    "            vit_emb = CPU_Unpickler(f).load() \n",
    "    if i > 36600 and i % 100 == 0:\n",
    "        try:\n",
    "            with open(\"../data/vit/vit_test_jpg_\" + str(i-1+100), \"rb\") as f: \n",
    "                vit_emb = CPU_Unpickler(f).load() \n",
    "        except:\n",
    "            vit_emb = [None] * 100\n",
    "    image_embedding = vit_emb[i % x]\n",
    "\n",
    "    tabular = torch.tensor([row[\"item_seq_number\"], row[\"day\"], row[\"month\"], row[\"year\"], row[\"weekday\"], row[\"dayofyear\"], user_type_dict[row[\"user_type\"]], 0.0 if row[\"price\"] is None else row[\"price\"]])\n",
    "    tabular = tabular.unsqueeze(0).unsqueeze(2).expand(-1, -1, 1024)\n",
    "    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "    tabular = F.normalize(tabular, dim=1, eps=1e-6)\n",
    "\n",
    "    text_embedding = jina_emb[i % 10000].unsqueeze(0)\n",
    "\n",
    "    if image_embedding is None:\n",
    "        image_embedding = torch.zeros(1, 1, 1024)\n",
    "    else:\n",
    "        if image_embedding.shape[0] != 1:\n",
    "            image_embedding = image_embedding.unsqueeze(0)\n",
    "        image_embedding = F.pad(image_embedding, (0, 1024-768), \"constant\", 0)\n",
    "    emb_concat = torch.concat((tabular.to(device), text_embedding.to(device), image_embedding.to(device)), dim=1)\n",
    "    y_pred.append(float(model(emb_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94efd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'item_id': item_id, 'deal_probability': np.clip(y_pred, 0, 1)}).to_csv(\"../results/feature-level-image-LSTM.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5caf7b8",
   "metadata": {},
   "source": [
    "Результат: 0.29387"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
