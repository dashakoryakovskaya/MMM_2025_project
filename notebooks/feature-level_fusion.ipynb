{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be41b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from typing import Tuple, Callable\n",
    "from torch.autograd import Function\n",
    "import gc\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee733cc-c412-4e38-98e7-b3cde5b35dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "    df['day'] = df['activation_date'].dt.day\n",
    "    df['month'] = df[\"activation_date\"].dt.month\n",
    "    df['year'] = df[\"activation_date\"].dt.year\n",
    "    df['weekday'] = df['activation_date'].dt.weekday\n",
    "    df[\"dayofyear\"] = df['activation_date'].dt.dayofyear\n",
    "    df.drop(columns=['activation_date', 'item_id'], inplace=True)\n",
    "    df['param_1'] = df['param_1'].fillna('')\n",
    "    df['param_2'] = df['param_2'].fillna('')\n",
    "    df['param_3'] = df['param_3'].fillna('')\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b5b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_avito(): \n",
    "    def __init__(self, part='train', len_1=15034, len_2=15034): \n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        train_1 = train[train.deal_probability != 0.0].iloc[0:len_1]\n",
    "        train_2 = train[train.deal_probability == 0.0].iloc[0:len_2]\n",
    "        #train = train.iloc[0:15034]\n",
    "        train = pd.concat([train_1, train_2])\n",
    "        train = preprocess(train)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['deal_probability', 'image']), train['deal_probability'], test_size=0.2, random_state=42)\n",
    "        self.x = X_train if part == 'train' else X_val\n",
    "        self.y = y_train if part == 'train' else y_val\n",
    "        self.n_samples = X_train.shape[0] if part == 'train' else X_val.shape[0]\n",
    "        self.text = list(self.x.apply(lambda item: '\\n'.join([ item[\"title\"], str(item[\"description\"]), item[\"region\"], item[\"city\"], item[\"parent_category_name\"], item[\"category_name\"], ('' if item[\"param_1\"] is None else str(item[\"param_1\"])), ('' if item[\"param_2\"] is None else str(item[\"param_2\"])), ('' if item[\"param_3\"] is None else str(item[\"param_3\"]))]), axis=1).values)\n",
    "        user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "        self.tabular = list(self.x.apply(lambda item: torch.tensor([item[\"item_seq_number\"], item[\"day\"], item[\"month\"], item[\"year\"], item[\"weekday\"], item[\"dayofyear\"], user_type_dict[item[\"user_type\"]], 0.0 if item[\"price\"] is None else item[\"price\"]]), axis=1).values)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        return self.tabular[index], self.text[index], np.array(self.y)[index] \n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3c03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train'), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val'), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985e24c3-33fc-41dd-87aa-58b91c08ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "@dataclass\n",
    "class ModelTrainer:\n",
    "    model: 'typing.Any'\n",
    "    train_dataloader: DataLoader\n",
    "    val_dataloader: DataLoader\n",
    "    device: torch.device\n",
    "    epochs: int\n",
    "    round_loss: int\n",
    "    round_rmse: int\n",
    "\n",
    "    optimizer: torch.optim\n",
    "    loss_fn: 'typing.Any'\n",
    "    \n",
    "    patience: int = 10 # Ранняя остановка обучения\n",
    "\n",
    "    def __post_init__(self):        \n",
    "        # История обучения и тестирования\n",
    "        self.__history = pd.DataFrame({\n",
    "            \"train_avg\": [], # Средние метрики на тренировочной выборке\n",
    "            \"val_avg\": [], # Средние метрики на валидационной выборке\n",
    "            \"train_loss\": [], # Loss на тренировочной выборке\n",
    "            \"val_loss\": [], # Loss на валидационной выборке\n",
    "        })\n",
    "\n",
    "        # Количество шагов в одной эпохе\n",
    "        self.__train_steps = len(self.train_dataloader)\n",
    "        self.__val_steps = len(self.val_dataloader)\n",
    "\n",
    "        self.__best_val_avg = 0\n",
    "        self.__no_improvement_count = 0\n",
    "        \n",
    "        self.loss_fn = self.loss_fn\n",
    "\n",
    "    @property\n",
    "    def history(self) -> pd.DataFrame:\n",
    "        \"\"\"Получение DataFrame историей обучения и тестирования\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: **DataFrame** c историей обучения и тестирования\n",
    "        \"\"\"\n",
    "\n",
    "        return self.__history\n",
    "\n",
    "    @classmethod\n",
    "    def _is_best_model(self, dev_avg: float) -> bool:\n",
    "        \"\"\"Проверка, является ли текущая модель лучшей на основе метрик валидации\n",
    "\n",
    "        Args:\n",
    "            test_accuracy (float): Текущая точность тестирования\n",
    "\n",
    "        Returns:\n",
    "            bool: True, если текущая модель лучшая, иначе False\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            min_val_avg = min(self.__history[\"val_avg\"])\n",
    "        except ValueError:\n",
    "            min_val_avg = 10**10\n",
    "        return dev_avg < min_val_avg\n",
    "\n",
    "    def _save_model(self, epoch: int, path_to_model: str, test_rmse: float, loss: torch.Tensor) -> None:\n",
    "        \"\"\"Сохранение модели\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Текущая эпоха\n",
    "            path_to_model (str): Путь для сохранения модели\n",
    "            test_rmse (float): rmse на тестовой выборке\n",
    "            loss (torch.Tensor): Значение потерь\n",
    "        \"\"\"\n",
    "        \n",
    "        os.makedirs(path_to_model, exist_ok = True)\n",
    "        self._best_model_name = f\"{self.model.__class__.__name__}_{epoch}_{test_rmse}_checkpoint.pth\"\n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"test_loss\": loss,\n",
    "        }, os.path.join(path_to_model, f\"{self.model.__class__.__name__}_{epoch}_{test_rmse}_checkpoint.pth\"))\n",
    "    \n",
    "    # Процесс обучения\n",
    "    def train(self, path_to_model: str) -> None:\n",
    "        \"\"\"Процесс обучения\n",
    "\n",
    "        Args:\n",
    "            path_to_model (str): Путь для сохранения моделей\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        losses_train_list = []\n",
    "        losses_val_list = []\n",
    "        rmse_train_list = []\n",
    "        rmse_val_list = []\n",
    "        min_val_rmse = 10**10\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            with torch.no_grad():\n",
    "                torch.cuda.empty_cache()\n",
    "            self.model.train() # Установка модели в режим обучения\n",
    "            # Сумма Loss\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            # Сумма rmse\n",
    "            train_rmse = 0\n",
    "            val_rmse = 0\n",
    "\n",
    "            # Проход по всем тренировочным пакетам\n",
    "            with tqdm(total = self.__train_steps, desc = f\"Эпоха {epoch}\", unit = \"batch\") as pbar_train:\n",
    "                for batch, (tabular, text, targets) in enumerate(self.train_dataloader, 1):\n",
    "                    tabular = tabular.unsqueeze(2).expand(-1, -1, 1024).to(device)\n",
    "                    tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "                    text_embedding = []\n",
    "                    for i in range(len(text)):\n",
    "                        encoded_input = feature_extractor_tokenizer(text[i], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                        with torch.no_grad():\n",
    "                            features = feature_extractor_model(**encoded_input)[0][0]\n",
    "                        text_embedding.append(features)\n",
    "                    text_embedding = torch.nn.utils.rnn.pad_sequence(text_embedding, batch_first=True)\n",
    "                    emb_concat = torch.concat((tabular, text_embedding), 1)\n",
    "                    emb_concat = emb_concat.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    logits = self.model(emb_concat)\n",
    "                    logits = torch.nan_to_num(logits, nan=0.0)\n",
    "                    loss = self.loss_fn(logits, targets.float()) # Ошибка предсказаний\n",
    "\n",
    "                    # Обратное распространение для обновления весов\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "        \n",
    "                    total_train_loss += loss.item() # Потеря\n",
    "                    # RMSE\n",
    "                    train_rmse += root_mean_squared_error(targets.cpu().detach().numpy(), logits.cpu().detach().numpy())\n",
    "        \n",
    "                    pbar_train.update(1)\n",
    "                    with torch.no_grad():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Средняя потеря\n",
    "                avg_train_loss = round(total_train_loss / batch, self.round_loss)\n",
    "                losses_train_list.append(avg_train_loss)\n",
    "        \n",
    "                # RMSE\n",
    "                train_rmse = round(train_rmse / len(self.train_dataloader.dataset) * 100, self.round_rmse)\n",
    "                rmse_train_list.append(train_rmse)\n",
    "        \n",
    "                pbar_train.set_postfix({\n",
    "                    \"rmse\": train_rmse,\n",
    "                    \"Средняя потеря\": avg_train_loss\n",
    "                })\n",
    "            \n",
    "            \n",
    "            # Установка модели в режим предсказаний\n",
    "            self.model.eval()\n",
    "        \n",
    "            # Предсказания на валидационной выборке\n",
    "            with torch.no_grad():\n",
    "                with tqdm(total = self.__val_steps, desc = f\"Тестирование {epoch}\", unit = \"batch\") as pbar_val:\n",
    "                    for batch, (tabular, text, targets) in enumerate(self.val_dataloader, 1):\n",
    "                        text_embedding = []\n",
    "                        tabular = tabular.unsqueeze(2).expand(-1, -1, 1024).to(device)\n",
    "                        tabular = torch.nan_to_num(tabular,nan=0.0)\n",
    "                        for i in range(len(text)):\n",
    "                            encoded_input = feature_extractor_tokenizer(text[i], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                            with torch.no_grad():\n",
    "                                features = feature_extractor_model(**encoded_input)[0][0]\n",
    "                            text_embedding.append(features)\n",
    "                        text_embedding = torch.nn.utils.rnn.pad_sequence(text_embedding, batch_first=True)\n",
    "                        emb_concat = torch.concat((tabular, text_embedding), 1)\n",
    "                        emb_concat = emb_concat.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        logits = self.model(emb_concat)\n",
    "                        logits = torch.nan_to_num(logits, nan=0.0)\n",
    "                        loss = self.loss_fn(logits, targets.float()) # Ошибка предсказаний\n",
    "                        \n",
    "                        total_val_loss += loss.item() # Потеря\n",
    "                        # RMSE\n",
    "                        val_rmse += root_mean_squared_error(targets.cpu().detach().numpy(), logits.cpu().detach().numpy())\n",
    "        \n",
    "                        pbar_val.update(1)\n",
    "                        with torch.no_grad():\n",
    "                            torch.cuda.empty_cache()\n",
    "                    # Средняя потеря\n",
    "                    avg_val_loss = round(total_val_loss / batch, self.round_loss)\n",
    "                    losses_val_list.append(avg_val_loss)\n",
    "        \n",
    "                    # RMSE\n",
    "                    val_rmse = round(val_rmse / len(self.val_dataloader.dataset) * 100, self.round_rmse)\n",
    "                    rmse_val_list.append(val_rmse)\n",
    "                    \n",
    "                    pbar_val.set_postfix({\n",
    "                        \"rmse\": val_rmse,\n",
    "                        \"Средняя потеря\": avg_val_loss\n",
    "                    })\n",
    "            \n",
    "            if val_rmse < min_val_rmse:\n",
    "                min_val_rmse = val_rmse\n",
    "                self._save_model(epoch, path_to_model, round(val_rmse, self.round_rmse), avg_val_loss)\n",
    "                self.__best_dev_avg = val_rmse\n",
    "                self.__no_improvement_count = 0\n",
    "            else:\n",
    "                self.__no_improvement_count += 1\n",
    "\n",
    "            if self.__no_improvement_count >= self.patience:\n",
    "                print(f\"Ранняя остановка на эпохе {epoch} из-за отсутствия улучшения точности на тестовой выборке\")\n",
    "                break\n",
    "\n",
    "    # Получение хэш-значения\n",
    "    def __hash__(self):\n",
    "        return id(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88a32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20 # Количество эпох\n",
    "BATCH_SIZE = 32 # Размер выборки (пакета)\n",
    "LEARNING_RATE = 1e-4 # Скорость обучения\n",
    "ROUND_RMSE = 2 # Знаков Accuracy после запятой\n",
    "ROUND_LOSS = 7 # Знаков Loss после запятой\n",
    "ROOT_DIR = os.path.join(\".\")\n",
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, \"Models_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31379769-0352-474f-adea-65d8843cea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True)\n",
    "feature_extractor_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aad5e68-3117-4b17-92da-548d34007112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim = 1024, hidden_dim=128, num_heads = 4, num_layers = 8, dropout = 0.1):\n",
    "        super(TransformerModelWithAttention, self).__init__()\n",
    "        self.in_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 10000, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = hidden_dim, nhead = num_heads, dim_feedforward = hidden_dim, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.in_layer(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "        encoder_output = self.transformer_encoder(x)\n",
    "        x = encoder_output.mean(dim = 1)\n",
    "        return torch.clamp(self.fc_out(x), 0.0, 1.0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c3fe71d6-7c1a-4d59-8b17-101d4c7f546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [06:31<00:00,  1.04s/batch, rmse=0.91, Средняя потеря=0.0893]\n",
      "Тестирование 1: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 2: 100%|██████████| 376/376 [06:27<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 2: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 3: 100%|██████████| 376/376 [06:26<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 3: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 4: 100%|██████████| 376/376 [06:23<00:00,  1.02s/batch, rmse=0.91, Средняя потеря=0.089]\n",
      "Тестирование 4: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 5: 100%|██████████| 376/376 [06:24<00:00,  1.02s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 5: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 6: 100%|██████████| 376/376 [06:43<00:00,  1.07s/batch, rmse=0.91, Средняя потеря=0.089]\n",
      "Тестирование 6: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 7: 100%|██████████| 376/376 [06:27<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 7: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 8: 100%|██████████| 376/376 [07:05<00:00,  1.13s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 8: 100%|██████████| 94/94 [01:36<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 9: 100%|██████████| 376/376 [06:27<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 9: 100%|██████████| 94/94 [02:00<00:00,  1.28s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 10: 100%|██████████| 376/376 [06:33<00:00,  1.05s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 10: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 11: 100%|██████████| 376/376 [06:23<00:00,  1.02s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 11: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 11 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(pooling=None,  num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b13cc-24e1-4f04-b465-b4291a1ef428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [06:42<00:00,  1.07s/batch, rmse=0.91, Средняя потеря=0.0896]\n",
      "Тестирование 1: 100%|██████████| 94/94 [02:02<00:00,  1.30s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 2: 100%|██████████| 376/376 [06:28<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.089]\n",
      "Тестирование 2: 100%|██████████| 94/94 [01:49<00:00,  1.16s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 3: 100%|██████████| 376/376 [11:18<00:00,  1.81s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 3: 100%|██████████| 94/94 [02:16<00:00,  1.45s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 4: 100%|██████████| 376/376 [06:25<00:00,  1.02s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 4: 100%|██████████| 94/94 [01:36<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 5: 100%|██████████| 376/376 [10:25<00:00,  1.66s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:15<00:00,  2.08s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 6: 100%|██████████| 376/376 [11:53<00:00,  1.90s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 6: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 7: 100%|██████████| 376/376 [06:25<00:00,  1.03s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 7: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 8: 100%|██████████| 376/376 [06:24<00:00,  1.02s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 8: 100%|██████████| 94/94 [01:35<00:00,  1.01s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 9:  79%|███████▊  | 296/376 [05:00<01:20,  1.00s/batch]"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46f9683-b778-4eca-b337-3cfdf35ae90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [08:31<00:00,  1.36s/batch, rmse=0.91, Средняя потеря=0.0891]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:20<00:00,  2.13s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 2: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:02<00:00,  2.08s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:15<00:00,  2.08s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 4: 100%|██████████| 376/376 [13:05<00:00,  2.09s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 5: 100%|██████████| 376/376 [13:26<00:00,  2.14s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:20<00:00,  2.14s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 6: 100%|██████████| 376/376 [13:05<00:00,  2.09s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:15<00:00,  2.08s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 8: 100%|██████████| 376/376 [13:09<00:00,  2.10s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 9: 100%|██████████| 376/376 [13:11<00:00,  2.10s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 9: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 10: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 10: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 11: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 11: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 11 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8902799-8264-44a3-88ca-e0718e134443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 2: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:16<00:00,  2.10s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 4: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 5: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 6: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 7: 100%|██████████| 376/376 [13:11<00:00,  2.10s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 7: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 8: 100%|██████████| 376/376 [13:11<00:00,  2.10s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 9: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 9: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 10: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.91, Средняя потеря=0.089]\n",
      "Тестирование 10: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]\n",
      "Эпоха 11: 100%|██████████| 376/376 [13:10<00:00,  2.10s/batch, rmse=0.91, Средняя потеря=0.0889]\n",
      "Тестирование 11: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 11 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85862290-9f8e-4da9-92e8-543e1472f0b3",
   "metadata": {},
   "source": [
    "новый dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e511dcb-d3df-4ea8-91c1-8c9d35dd147c",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14027ff5-b221-4107-bc07-b06d3598e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [10:22<00:00,  1.66s/batch, rmse=1.09, Средняя потеря=0.125]\n",
      "Тестирование 1: 100%|██████████| 94/94 [02:33<00:00,  1.63s/batch, rmse=1.06, Средняя потеря=0.12]\n",
      "Эпоха 2: 100%|██████████| 376/376 [10:14<00:00,  1.64s/batch, rmse=1.09, Средняя потеря=0.125]\n",
      "Тестирование 2: 100%|██████████| 94/94 [02:31<00:00,  1.61s/batch, rmse=1.06, Средняя потеря=0.12]\n",
      "Эпоха 3: 100%|██████████| 376/376 [10:09<00:00,  1.62s/batch, rmse=1.09, Средняя потеря=0.125]\n",
      "Тестирование 3: 100%|██████████| 94/94 [02:31<00:00,  1.61s/batch, rmse=1.06, Средняя потеря=0.12]\n",
      "Эпоха 4:  91%|█████████ | 343/376 [09:07<00:53,  1.62s/batch]"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a0a56-f7b1-4e2d-a296-33c0979ae273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 752/752 [12:55<00:00,  1.03s/batch, rmse=1.09, Средняя потеря=0.124]\n",
      "Тестирование 1: 100%|██████████| 188/188 [03:12<00:00,  1.02s/batch, rmse=1.09, Средняя потеря=0.126]\n",
      "Эпоха 2: 100%|██████████| 752/752 [13:03<00:00,  1.04s/batch, rmse=1.08, Средняя потеря=0.124]\n",
      "Тестирование 2: 100%|██████████| 188/188 [03:13<00:00,  1.03s/batch, rmse=1.09, Средняя потеря=0.126]\n",
      "Эпоха 3: 100%|██████████| 752/752 [12:57<00:00,  1.03s/batch, rmse=1.08, Средняя потеря=0.124]\n",
      "Тестирование 3: 100%|██████████| 188/188 [03:12<00:00,  1.02s/batch, rmse=1.09, Средняя потеря=0.126]\n",
      "Эпоха 4:  57%|█████▋    | 429/752 [07:22<05:36,  1.04s/batch]"
     ]
    }
   ],
   "source": [
    "model_transformer = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_transformer.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_transformer, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ffb57-6b21-4d00-af9e-f6d55258a555",
   "metadata": {},
   "source": [
    "## Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "973e89ab-ab94-4c19-9a6f-18a226b73cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ff0bde-186b-4f18-bdad-0cd51327d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, \"Models_mamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31299bfa-4c7a-4c42-a0df-8e28746e1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import silu\n",
    "from torch.nn.functional import softplus\n",
    "from einops import rearrange, repeat, einsum\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:        \n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps) * self.weight\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, num_layers, d_input, d_model, d_state=16, d_discr=None, ker_size=4):\n",
    "        super().__init__()\n",
    "        mamba_par = {\n",
    "            'd_input' : d_input,\n",
    "            'd_model' : d_model,\n",
    "            'd_state' : d_state,\n",
    "            'd_discr' : d_discr,\n",
    "            'ker_size': ker_size\n",
    "        }\n",
    "        self.layers = nn.ModuleList([nn.ModuleList([MambaBlock(**mamba_par), RMSNorm(d_input)]) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_input, 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        seq = seq.to(self.device)\n",
    "        for mamba, norm in self.layers:\n",
    "            out, cache = mamba(norm(seq), cache)\n",
    "            seq = out + seq\n",
    "        return self.fc_out(seq.mean(dim = 1))\n",
    "        \n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model, d_state=16, d_discr=None, ker_size=4):\n",
    "        super().__init__()\n",
    "        d_discr = d_discr if d_discr is not None else d_model // 16\n",
    "        self.in_proj  = nn.Linear(d_input, 2 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_input, bias=False)\n",
    "        self.s_B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_C = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.s_D = nn.Sequential(nn.Linear(d_model, d_discr, bias=False), nn.Linear(d_discr, d_model, bias=False),)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=ker_size,\n",
    "            padding=ker_size - 1,\n",
    "            groups=d_model,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.arange(1, d_state + 1, dtype=torch.float).repeat(d_model, 1))\n",
    "        self.D = nn.Parameter(torch.ones(d_model, dtype=torch.float))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, seq, cache=None):\n",
    "        b, l, d = seq.shape\n",
    "        (prev_hid, prev_inp) = cache if cache is not None else (None, None)\n",
    "        a, b = self.in_proj(seq).chunk(2, dim=-1)\n",
    "        x = rearrange(a, 'b l d -> b d l')\n",
    "        x = x if prev_inp is None else torch.cat((prev_inp, x), dim=-1)\n",
    "        a = self.conv(x)[..., :l]\n",
    "        a = rearrange(a, 'b d l -> b l d')\n",
    "        a = silu(a)\n",
    "        a, hid = self.ssm(a, prev_hid=prev_hid) \n",
    "        b = silu(b)\n",
    "        out = a * b\n",
    "        out =  self.out_proj(out)\n",
    "        if cache:\n",
    "            cache = (hid.squeeze(), x[..., 1:])   \n",
    "        return out, cache\n",
    "    \n",
    "    def ssm(self, seq, prev_hid):\n",
    "        A = -self.A\n",
    "        D = +self.D\n",
    "        B = self.s_B(seq)\n",
    "        C = self.s_C(seq)\n",
    "        s = softplus(D + self.s_D(seq))\n",
    "        A_bar = einsum(torch.exp(A), s, 'd s,   b l d -> b l d s')\n",
    "        B_bar = einsum(          B,  s, 'b l s, b l d -> b l d s')\n",
    "        X_bar = einsum(B_bar, seq, 'b l d s, b l d -> b l d s')\n",
    "        hid = self._hid_states(A_bar, X_bar, prev_hid=prev_hid)\n",
    "        out = einsum(hid, C, 'b l d s, b l s -> b l d')\n",
    "        out = out + D * seq\n",
    "        return out, hid\n",
    "    \n",
    "    def _hid_states(self, A, X, prev_hid=None):\n",
    "        b, l, d, s = A.shape\n",
    "        A = rearrange(A, 'b l d s -> l b d s')\n",
    "        X = rearrange(X, 'b l d s -> l b d s')\n",
    "        if prev_hid is not None:\n",
    "            return rearrange(A * prev_hid + X, 'l b d s -> b l d s')\n",
    "        h = torch.zeros(b, d, s, device=self.device)\n",
    "        return torch.stack([h := A_t * h + X_t for A_t, X_t in zip(A, X)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1b565-e438-46aa-9a8a-d1cc937cfb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [13:47<00:00,  2.20s/batch, rmse=114, Средняя потеря=9.14e+3]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=158, Средняя потеря=2.69e+4]\n",
      "Эпоха 2: 100%|██████████| 376/376 [13:38<00:00,  2.18s/batch, rmse=70.6, Средняя потеря=2.31e+3]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=170, Средняя потеря=3.12e+4]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:59<00:00,  2.23s/batch, rmse=61.3, Средняя потеря=1.28e+3]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:06<00:00,  1.99s/batch, rmse=66.1, Средняя потеря=4.7e+3]\n",
      "Эпоха 4: 100%|██████████| 376/376 [14:21<00:00,  2.29s/batch, rmse=73.5, Средняя потеря=2e+3]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:25<00:00,  2.19s/batch, rmse=61.3, Средняя потеря=4.03e+3]\n",
      "Эпоха 5: 100%|██████████| 376/376 [15:17<00:00,  2.44s/batch, rmse=61.9, Средняя потеря=1.99e+3]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:23<00:00,  2.17s/batch, rmse=7.94, Средняя потеря=67.2]\n",
      "Эпоха 6: 100%|██████████| 376/376 [14:31<00:00,  2.32s/batch, rmse=64.9, Средняя потеря=1.82e+3]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:15<00:00,  2.08s/batch, rmse=39.2, Средняя потеря=1.64e+3]\n",
      "Эпоха 7: 100%|██████████| 376/376 [14:14<00:00,  2.27s/batch, rmse=68.4, Средняя потеря=1.76e+3]\n",
      "Тестирование 7: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=172, Средняя потеря=3.18e+4]\n",
      "Эпоха 8: 100%|██████████| 376/376 [14:11<00:00,  2.26s/batch, rmse=72.7, Средняя потеря=3.17e+3]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:15<00:00,  2.08s/batch, rmse=181, Средняя потеря=3.52e+4]\n",
      "Эпоха 9: 100%|██████████| 376/376 [14:05<00:00,  2.25s/batch, rmse=73.3, Средняя потеря=3.32e+3]\n",
      "Тестирование 9: 100%|██████████| 94/94 [03:24<00:00,  2.17s/batch, rmse=173, Средняя потеря=3.22e+4]\n",
      "Эпоха 10: 100%|██████████| 376/376 [14:11<00:00,  2.26s/batch, rmse=76.1, Средняя потеря=4.36e+3]\n",
      "Тестирование 10: 100%|██████████| 94/94 [03:18<00:00,  2.12s/batch, rmse=63.4, Средняя потеря=4.34e+3]\n",
      "Эпоха 11: 100%|██████████| 376/376 [14:04<00:00,  2.25s/batch, rmse=69.4, Средняя потеря=3.46e+3]\n",
      "Тестирование 11: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=4.28, Средняя потеря=16.9]\n",
      "Эпоха 12: 100%|██████████| 376/376 [14:05<00:00,  2.25s/batch, rmse=70, Средняя потеря=4.25e+3]\n",
      "Тестирование 12: 100%|██████████| 94/94 [03:23<00:00,  2.16s/batch, rmse=199, Средняя потеря=4.28e+4]\n",
      "Эпоха 13: 100%|██████████| 376/376 [14:32<00:00,  2.32s/batch, rmse=74.5, Средняя потеря=5.31e+3]\n",
      "Тестирование 13: 100%|██████████| 94/94 [03:24<00:00,  2.18s/batch, rmse=159, Средняя потеря=2.71e+4]\n",
      "Эпоха 14:  42%|████▏     | 159/376 [06:08<08:38,  2.39s/batch]"
     ]
    }
   ],
   "source": [
    "model_mamba = Mamba(num_layers=2, d_input=1024, d_model=128).to(device)\n",
    "optimizer = optim.Adam(params = model_mamba.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_mamba, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf89cb-984e-4b7d-9707-586a34830c89",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c322d5-a49c-4bcb-8e62-a67de0e0abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 1024, hidden_size = 64, num_layers = 2, dropout = 0.1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.lstm.bidirectional:\n",
    "            h0, c0 = torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        else:\n",
    "            h0, c0 = torch.zeros(self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        if self.lstm.bidirectional:\n",
    "            out = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72df8784-a3ec-4fb5-9ddc-b665148f9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, \"Models_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba66b-a353-478c-8907-9fff54b9256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [25:15<00:00,  4.03s/batch, rmse=0.91, Средняя потеря=0.0858]\n",
      "Тестирование 1: 100%|██████████| 94/94 [07:48<00:00,  4.98s/batch, rmse=0.89, Средняя потеря=0.0836]\n",
      "Эпоха 2: 100%|██████████| 376/376 [28:05<00:00,  4.48s/batch, rmse=0.9, Средняя потеря=0.0854]\n",
      "Тестирование 2: 100%|██████████| 94/94 [06:45<00:00,  4.31s/batch, rmse=0.89, Средняя потеря=0.0832]\n",
      "Эпоха 3: 100%|██████████| 376/376 [27:07<00:00,  4.33s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 3: 100%|██████████| 94/94 [06:33<00:00,  4.19s/batch, rmse=0.89, Средняя потеря=0.083]\n",
      "Эпоха 4: 100%|██████████| 376/376 [26:43<00:00,  4.27s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 4: 100%|██████████| 94/94 [06:47<00:00,  4.34s/batch, rmse=0.89, Средняя потеря=0.0835]\n",
      "Эпоха 5: 100%|██████████| 376/376 [27:22<00:00,  4.37s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 5: 100%|██████████| 94/94 [06:53<00:00,  4.40s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 6: 100%|██████████| 376/376 [27:43<00:00,  4.43s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 6: 100%|██████████| 94/94 [06:58<00:00,  4.45s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 7: 100%|██████████| 376/376 [28:02<00:00,  4.47s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 7: 100%|██████████| 94/94 [06:54<00:00,  4.41s/batch, rmse=0.89, Средняя потеря=0.0836]\n",
      "Эпоха 8: 100%|██████████| 376/376 [27:28<00:00,  4.39s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 8: 100%|██████████| 94/94 [06:49<00:00,  4.36s/batch, rmse=0.89, Средняя потеря=0.083]\n",
      "Эпоха 9:  77%|███████▋  | 289/376 [21:07<06:22,  4.39s/batch]"
     ]
    }
   ],
   "source": [
    "model_lstm = LSTM().to(device)\n",
    "optimizer = optim.Adam(params = model_lstm.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = ModelTrainer(model_lstm, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
