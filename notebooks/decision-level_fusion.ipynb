{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be41b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "# Импорт необходимых библиотек\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from typing import Tuple, Callable\n",
    "from torch.autograd import Function\n",
    "import gc\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff5e75-b218-42a6-bcb4-b61ad02f11c5",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
    "\n",
    "    df['day'] = df['activation_date'].dt.day\n",
    "    df['month'] = df[\"activation_date\"].dt.month\n",
    "    df['year'] = df[\"activation_date\"].dt.year\n",
    "    df['weekday'] = df['activation_date'].dt.weekday\n",
    "    df[\"dayofyear\"] = df['activation_date'].dt.dayofyear\n",
    "    df.drop(columns=['activation_date', 'item_id'], inplace=True)\n",
    "    df['param_1'] = df['param_1'].fillna('')\n",
    "    df['param_2'] = df['param_2'].fillna('')\n",
    "    df['param_3'] = df['param_3'].fillna('')\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b5b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_avito(): \n",
    "    def __init__(self, part='train', len_1=15034, len_2=15034): \n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        train_1 = train[train.deal_probability != 0.0].iloc[0:len_1]\n",
    "        train_2 = train[train.deal_probability == 0.0].iloc[0:len_2]\n",
    "        #train = train.iloc[0:15034]\n",
    "        train = pd.concat([train_1, train_2])\n",
    "        train = preprocess(train)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['deal_probability', 'image']), train['deal_probability'], test_size=0.2, random_state=42)\n",
    "        self.x = X_train if part == 'train' else X_val\n",
    "        self.y = y_train if part == 'train' else y_val\n",
    "        self.n_samples = X_train.shape[0] if part == 'train' else X_val.shape[0]\n",
    "        self.text = list(self.x.apply(lambda item: '\\n'.join([ item[\"title\"], str(item[\"description\"]), item[\"region\"], item[\"city\"], item[\"parent_category_name\"], item[\"category_name\"], ('' if item[\"param_1\"] is None else str(item[\"param_1\"])), ('' if item[\"param_2\"] is None else str(item[\"param_2\"])), ('' if item[\"param_3\"] is None else str(item[\"param_3\"]))]), axis=1).values)\n",
    "        user_type_dict = {'Private': 0, 'Company': 1, 'Shop': 2}\n",
    "        self.tabular = list(self.x.apply(lambda item: torch.tensor([item[\"item_seq_number\"], item[\"day\"], item[\"month\"], item[\"year\"], item[\"weekday\"], item[\"dayofyear\"], user_type_dict[item[\"user_type\"]], 0.0 if item[\"price\"] is None else item[\"price\"]]), axis=1).values)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        return self.tabular[index], self.text[index], np.array(self.y)[index] \n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3c03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train'), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val'), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64aaab8-169e-40cc-9c47-82f7675637a8",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "@dataclass\n",
    "class ModelTrainer:\n",
    "    model: 'typing.Any'\n",
    "    train_dataloader: DataLoader\n",
    "    val_dataloader: DataLoader\n",
    "    device: torch.device\n",
    "    epochs: int\n",
    "    round_loss: int\n",
    "    round_rmse: int\n",
    "\n",
    "    optimizer: torch.optim\n",
    "    loss_fn: 'typing.Any'\n",
    "    \n",
    "    step: str = \"tabular\"\n",
    "    \n",
    "    patience: int = 10 # Ранняя остановка обучения\n",
    "\n",
    "    def __post_init__(self):        \n",
    "        # История обучения и тестирования\n",
    "        self.__history = pd.DataFrame({\n",
    "            \"train_avg\": [], # Средние метрики на тренировочной выборке\n",
    "            \"val_avg\": [], # Средние метрики на валидационной выборке\n",
    "            \"train_loss\": [], # Loss на тренировочной выборке\n",
    "            \"val_loss\": [], # Loss на валидационной выборке\n",
    "        })\n",
    "\n",
    "        # Количество шагов в одной эпохе\n",
    "        self.__train_steps = len(self.train_dataloader)\n",
    "        self.__val_steps = len(self.val_dataloader)\n",
    "\n",
    "        self.__best_val_avg = 0\n",
    "        self.__no_improvement_count = 0\n",
    "        \n",
    "        self.loss_fn = self.loss_fn\n",
    "\n",
    "    @property\n",
    "    def history(self) -> pd.DataFrame:\n",
    "        \"\"\"Получение DataFrame историей обучения и тестирования\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: **DataFrame** c историей обучения и тестирования\n",
    "        \"\"\"\n",
    "\n",
    "        return self.__history\n",
    "\n",
    "    @classmethod\n",
    "    def _is_best_model(self, dev_avg: float) -> bool:\n",
    "        \"\"\"Проверка, является ли текущая модель лучшей на основе метрик валидации\n",
    "\n",
    "        Args:\n",
    "            test_accuracy (float): Текущая точность тестирования\n",
    "\n",
    "        Returns:\n",
    "            bool: True, если текущая модель лучшая, иначе False\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            min_val_avg = min(self.__history[\"val_avg\"])\n",
    "        except ValueError:\n",
    "            min_val_avg = 10**10\n",
    "        return dev_avg < min_val_avg\n",
    "\n",
    "    def _save_model(self, epoch: int, path_to_model: str, test_rmse: float, loss: torch.Tensor) -> None:\n",
    "        \"\"\"Сохранение модели\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Текущая эпоха\n",
    "            path_to_model (str): Путь для сохранения модели\n",
    "            test_rmse (float): rmse на тестовой выборке\n",
    "            loss (torch.Tensor): Значение потерь\n",
    "        \"\"\"\n",
    "        \n",
    "        os.makedirs(path_to_model, exist_ok = True)\n",
    "        self._best_model_name = f\"{self.model.__class__.__name__}_{epoch}_{test_rmse}_checkpoint.pth\"\n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"test_loss\": loss,\n",
    "        }, os.path.join(path_to_model, f\"{self.model.__class__.__name__}_{epoch}_{test_rmse}_checkpoint.pth\"))\n",
    "    \n",
    "    # Процесс обучения\n",
    "    def train(self, path_to_model: str) -> None:\n",
    "        \"\"\"Процесс обучения\n",
    "\n",
    "        Args:\n",
    "            path_to_model (str): Путь для сохранения моделей\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        losses_train_list = []\n",
    "        losses_val_list = []\n",
    "        rmse_train_list = []\n",
    "        rmse_val_list = []\n",
    "        min_val_rmse = 10**10\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            with torch.no_grad():\n",
    "                torch.cuda.empty_cache()\n",
    "            self.model.train() # Установка модели в режим обучения\n",
    "            # Сумма Loss\n",
    "            total_train_loss = 0\n",
    "            total_val_loss = 0\n",
    "            # Сумма rmse\n",
    "            train_rmse = 0\n",
    "            val_rmse = 0\n",
    "\n",
    "            # Проход по всем тренировочным пакетам\n",
    "            with tqdm(total = self.__train_steps, desc = f\"Эпоха {epoch}\", unit = \"batch\") as pbar_train:\n",
    "                for batch, (tabular, text, targets) in enumerate(self.train_dataloader, 1):\n",
    "                    if step == \"tabular\":\n",
    "                        x = tabular.to(device)\n",
    "                        x = torch.nan_to_num(x, nan=0.5)\n",
    "                    elif step == \"text\":\n",
    "                        text_embedding = []\n",
    "                        for i in range(len(text)):\n",
    "                            encoded_input = feature_extractor_tokenizer(text[i], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                            with torch.no_grad():\n",
    "                                features = feature_extractor_model(**encoded_input)[0][0]\n",
    "                            text_embedding.append(features.float())\n",
    "                        x = torch.nn.utils.rnn.pad_sequence(text_embedding, batch_first=True)\n",
    "                        x = x.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    logits = self.model(x)\n",
    "                    if logits.isnan().sum() != 0:\n",
    "                        print(\"logits\")\n",
    "                    logits = torch.nan_to_num(logits, nan=0.0)\n",
    "                    loss = self.loss_fn(logits, targets.float()) # Ошибка предсказаний\n",
    "\n",
    "                    # Обратное распространение для обновления весов\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "        \n",
    "                    total_train_loss += loss.item() # Потеря\n",
    "                    # RMSE\n",
    "                    train_rmse += root_mean_squared_error(targets.cpu().detach().numpy(), logits.cpu().detach().numpy())\n",
    "        \n",
    "                    pbar_train.update(1)\n",
    "                    with torch.no_grad():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Средняя потеря\n",
    "                avg_train_loss = round(total_train_loss / batch, self.round_loss)\n",
    "                losses_train_list.append(avg_train_loss)\n",
    "        \n",
    "                # RMSE\n",
    "                train_rmse = round(train_rmse / len(self.train_dataloader.dataset) * 100, self.round_rmse)\n",
    "                rmse_train_list.append(train_rmse)\n",
    "        \n",
    "                pbar_train.set_postfix({\n",
    "                    \"rmse\": train_rmse,\n",
    "                    \"Средняя потеря\": avg_train_loss\n",
    "                })\n",
    "            \n",
    "            \n",
    "            # Установка модели в режим предсказаний\n",
    "            self.model.eval()\n",
    "        \n",
    "            # Предсказания на валидационной выборке\n",
    "            with torch.no_grad():\n",
    "                with tqdm(total = self.__val_steps, desc = f\"Тестирование {epoch}\", unit = \"batch\") as pbar_val:\n",
    "                    for batch, (tabular, text, targets) in enumerate(self.val_dataloader, 1):\n",
    "                        if step == \"tabular\":\n",
    "                            x = tabular.to(device)\n",
    "                            x = torch.nan_to_num(x, nan=0.0)\n",
    "                            #x = x.unsqueeze(dim=1)\n",
    "                        elif step == \"text\":\n",
    "                            text_embedding = []\n",
    "                            for i in range(len(text)):\n",
    "                                encoded_input = feature_extractor_tokenizer(text[i], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                                with torch.no_grad():\n",
    "                                    features = feature_extractor_model(**encoded_input)[0][0]\n",
    "                                text_embedding.append(features.float())\n",
    "                            x = torch.nn.utils.rnn.pad_sequence(text_embedding, batch_first=True)\n",
    "                            x = x.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        logits = self.model(x)\n",
    "                        logits = torch.nan_to_num(logits, nan=0.0)\n",
    "                        loss = self.loss_fn(logits, targets.float()) # Ошибка предсказаний\n",
    "                        \n",
    "                        total_val_loss += loss.item() # Потеря\n",
    "                        # RMSE\n",
    "                        val_rmse += root_mean_squared_error(targets.cpu().detach().numpy(), logits.cpu().detach().numpy())\n",
    "        \n",
    "                        pbar_val.update(1)\n",
    "                        with torch.no_grad():\n",
    "                            torch.cuda.empty_cache()\n",
    "                    # Средняя потеря\n",
    "                    avg_val_loss = round(total_val_loss / batch, self.round_loss)\n",
    "                    losses_val_list.append(avg_val_loss)\n",
    "        \n",
    "                    # RMSE\n",
    "                    val_rmse = round(val_rmse / len(self.val_dataloader.dataset) * 100, self.round_rmse)\n",
    "                    rmse_val_list.append(val_rmse)\n",
    "                    \n",
    "                    pbar_val.set_postfix({\n",
    "                        \"rmse\": val_rmse,\n",
    "                        \"Средняя потеря\": avg_val_loss\n",
    "                    })\n",
    "            \n",
    "            if val_rmse < min_val_rmse:\n",
    "                min_val_rmse = val_rmse\n",
    "                self._save_model(epoch, path_to_model, round(val_rmse, self.round_rmse), avg_val_loss)\n",
    "                self.__best_dev_avg = val_rmse\n",
    "                self.__no_improvement_count = 0\n",
    "            else:\n",
    "                self.__no_improvement_count += 1\n",
    "\n",
    "            if self.__no_improvement_count >= self.patience:\n",
    "                print(f\"Ранняя остановка на эпохе {epoch} из-за отсутствия улучшения точности на тестовой выборке\")\n",
    "                break\n",
    "\n",
    "    # Получение хэш-значения\n",
    "    def __hash__(self):\n",
    "        return id(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88a32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20 # Количество эпох\n",
    "BATCH_SIZE = 32 # Размер выборки (пакета)\n",
    "LEARNING_RATE = 1e-4 # Скорость обучения\n",
    "ROUND_RMSE = 2 # Знаков Accuracy после запятой\n",
    "ROUND_LOSS = 7 # Знаков Loss после запятой\n",
    "ROOT_DIR = os.path.join(\".\")\n",
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, \"Models_transformer_decision-level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb883f-68c5-4270-beb5-a75c60a1da2e",
   "metadata": {},
   "source": [
    "Экстрактор признаков из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31379769-0352-474f-adea-65d8843cea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True)\n",
    "feature_extractor_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ddb168-e371-49ed-b8ce-9783979abaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim = 1024, hidden_dim=128, num_heads = 4, num_layers = 8, dropout = 0.1):\n",
    "        super(TransformerModelWithAttention, self).__init__()\n",
    "        self.in_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 10000, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = hidden_dim, nhead = num_heads, dim_feedforward = hidden_dim, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.in_layer(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "        encoder_output = self.transformer_encoder(x)\n",
    "        x = encoder_output.mean(dim = 1)\n",
    "        return self.fc_out(x).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711ae69-7631-41fc-8cfa-37a4301014d4",
   "metadata": {},
   "source": [
    "### Предсказание на табличных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d6d3f7a-f8fb-43da-8f21-c2dc061ac6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 1880/1880 [00:16<00:00, 112.28batch/s, rmse=71.6, Средняя потеря=1.68e+4]\n",
      "Тестирование 1: 100%|██████████| 470/470 [00:01<00:00, 290.71batch/s, rmse=1.09, Средняя потеря=0.145]\n",
      "Эпоха 2: 100%|██████████| 1880/1880 [00:16<00:00, 116.46batch/s, rmse=1.02, Средняя потеря=0.165]\n",
      "Тестирование 2: 100%|██████████| 470/470 [00:01<00:00, 282.14batch/s, rmse=0.86, Средняя потеря=0.0785]\n",
      "Эпоха 3: 100%|██████████| 1880/1880 [00:16<00:00, 113.63batch/s, rmse=0.88, Средняя потеря=0.0872]\n",
      "Тестирование 3: 100%|██████████| 470/470 [00:01<00:00, 290.05batch/s, rmse=0.87, Средняя потеря=0.0846]\n",
      "Эпоха 4: 100%|██████████| 1880/1880 [00:16<00:00, 114.04batch/s, rmse=0.87, Средняя потеря=0.0822]\n",
      "Тестирование 4: 100%|██████████| 470/470 [00:02<00:00, 210.17batch/s, rmse=0.83, Средняя потеря=0.0737]\n",
      "Эпоха 5: 100%|██████████| 1880/1880 [00:16<00:00, 115.90batch/s, rmse=0.85, Средняя потеря=0.0766]\n",
      "Тестирование 5: 100%|██████████| 470/470 [00:01<00:00, 286.61batch/s, rmse=0.85, Средняя потеря=0.0775]\n",
      "Эпоха 6: 100%|██████████| 1880/1880 [00:16<00:00, 115.64batch/s, rmse=0.85, Средняя потеря=0.0774]\n",
      "Тестирование 6: 100%|██████████| 470/470 [00:01<00:00, 281.41batch/s, rmse=0.83, Средняя потеря=0.0723]\n",
      "Эпоха 7: 100%|██████████| 1880/1880 [00:16<00:00, 112.95batch/s, rmse=0.83, Средняя потеря=0.0735]\n",
      "Тестирование 7: 100%|██████████| 470/470 [00:01<00:00, 288.05batch/s, rmse=0.81, Средняя потеря=0.0708]\n",
      "Эпоха 8: 100%|██████████| 1880/1880 [00:16<00:00, 111.25batch/s, rmse=0.83, Средняя потеря=0.0727]\n",
      "Тестирование 8: 100%|██████████| 470/470 [00:01<00:00, 286.88batch/s, rmse=0.83, Средняя потеря=0.0747]\n",
      "Эпоха 9: 100%|██████████| 1880/1880 [00:16<00:00, 115.78batch/s, rmse=0.83, Средняя потеря=0.0721]\n",
      "Тестирование 9: 100%|██████████| 470/470 [00:01<00:00, 291.47batch/s, rmse=0.83, Средняя потеря=0.0715]\n",
      "Эпоха 10: 100%|██████████| 1880/1880 [00:16<00:00, 115.79batch/s, rmse=0.82, Средняя потеря=0.0718]\n",
      "Тестирование 10: 100%|██████████| 470/470 [00:01<00:00, 276.28batch/s, rmse=0.82, Средняя потеря=0.0702]\n",
      "Эпоха 11: 100%|██████████| 1880/1880 [00:16<00:00, 111.10batch/s, rmse=0.82, Средняя потеря=0.0707]\n",
      "Тестирование 11: 100%|██████████| 470/470 [00:01<00:00, 290.00batch/s, rmse=0.81, Средняя потеря=0.0701]\n",
      "Эпоха 12: 100%|██████████| 1880/1880 [00:16<00:00, 115.29batch/s, rmse=0.82, Средняя потеря=0.0707]\n",
      "Тестирование 12: 100%|██████████| 470/470 [00:01<00:00, 276.41batch/s, rmse=0.81, Средняя потеря=0.0715]\n",
      "Эпоха 13: 100%|██████████| 1880/1880 [00:16<00:00, 115.44batch/s, rmse=0.82, Средняя потеря=0.0704]\n",
      "Тестирование 13: 100%|██████████| 470/470 [00:01<00:00, 294.15batch/s, rmse=0.82, Средняя потеря=0.0702]\n",
      "Эпоха 14: 100%|██████████| 1880/1880 [00:16<00:00, 115.35batch/s, rmse=0.81, Средняя потеря=0.0699]\n",
      "Тестирование 14: 100%|██████████| 470/470 [00:01<00:00, 294.23batch/s, rmse=0.84, Средняя потеря=0.0731]\n",
      "Эпоха 15: 100%|██████████| 1880/1880 [00:17<00:00, 109.21batch/s, rmse=0.81, Средняя потеря=0.07]\n",
      "Тестирование 15: 100%|██████████| 470/470 [00:01<00:00, 292.57batch/s, rmse=0.83, Средняя потеря=0.0717]\n",
      "Эпоха 16: 100%|██████████| 1880/1880 [00:16<00:00, 114.85batch/s, rmse=0.81, Средняя потеря=0.0697]\n",
      "Тестирование 16: 100%|██████████| 470/470 [00:01<00:00, 288.17batch/s, rmse=0.82, Средняя потеря=0.0695]\n",
      "Эпоха 17: 100%|██████████| 1880/1880 [00:16<00:00, 113.40batch/s, rmse=0.81, Средняя потеря=0.0699]\n",
      "Тестирование 17: 100%|██████████| 470/470 [00:01<00:00, 291.42batch/s, rmse=0.81, Средняя потеря=0.0703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 17 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_tabular = model = nn.Sequential(\n",
    "    nn.Linear(8, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ").to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"tabular\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd37fb-c4e0-486d-80f3-cf1da850814e",
   "metadata": {},
   "source": [
    "### Предсказание на текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dcc15f-1e91-4f9c-8268-d5882b86bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', train_len=15034), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', train_len=15034), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f408913-c442-4bea-bacd-a47e53bd3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [13:25<00:00,  2.14s/batch, rmse=0.85, Средняя потеря=0.0759]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:18<00:00,  2.11s/batch, rmse=0.77, Средняя потеря=0.0633]\n",
      "Эпоха 2: 100%|██████████| 376/376 [13:23<00:00,  2.14s/batch, rmse=0.78, Средняя потеря=0.065]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:18<00:00,  2.11s/batch, rmse=0.76, Средняя потеря=0.061]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:20<00:00,  2.13s/batch, rmse=0.77, Средняя потеря=0.0618]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:18<00:00,  2.11s/batch, rmse=0.74, Средняя потеря=0.058]\n",
      "Эпоха 4: 100%|██████████| 376/376 [13:43<00:00,  2.19s/batch, rmse=0.76, Средняя потеря=0.0606]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:25<00:00,  2.18s/batch, rmse=0.74, Средняя потеря=0.058]\n",
      "Эпоха 5: 100%|██████████| 376/376 [13:24<00:00,  2.14s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:18<00:00,  2.11s/batch, rmse=0.74, Средняя потеря=0.0579]\n",
      "Эпоха 6: 100%|██████████| 376/376 [13:22<00:00,  2.13s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:17<00:00,  2.11s/batch, rmse=0.75, Средняя потеря=0.059]\n",
      "Эпоха 7: 100%|██████████| 376/376 [13:18<00:00,  2.12s/batch, rmse=0.74, Средняя потеря=0.0579]\n",
      "Тестирование 7: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.74, Средняя потеря=0.0574]\n",
      "Эпоха 8: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.73, Средняя потеря=0.0568]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Эпоха 9: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.72, Средняя потеря=0.0552]\n",
      "Тестирование 9: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.75, Средняя потеря=0.0596]\n",
      "Эпоха 10: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.71, Средняя потеря=0.0538]\n",
      "Тестирование 10: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.76, Средняя потеря=0.0601]\n",
      "Эпоха 11: 100%|██████████| 376/376 [13:15<00:00,  2.11s/batch, rmse=0.7, Средняя потеря=0.0526]\n",
      "Тестирование 11: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.77, Средняя потеря=0.0631]\n",
      "Эпоха 12: 100%|██████████| 376/376 [13:15<00:00,  2.12s/batch, rmse=0.69, Средняя потеря=0.0507]\n",
      "Тестирование 12: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.76, Средняя потеря=0.0618]\n",
      "Эпоха 13: 100%|██████████| 376/376 [13:15<00:00,  2.12s/batch, rmse=0.68, Средняя потеря=0.0491]\n",
      "Тестирование 13: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.76, Средняя потеря=0.0614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 13 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_tabular = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42205247-81a0-45e2-99b2-3ac28624aa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [13:16<00:00,  2.12s/batch, rmse=0.82, Средняя потеря=0.0721]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.77, Средняя потеря=0.0626]\n",
      "Эпоха 2: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.78, Средняя потеря=0.0636]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.82, Средняя потеря=0.069]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.76, Средняя потеря=0.0617]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.76, Средняя потеря=0.0612]\n",
      "Эпоха 4: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.76, Средняя потеря=0.061]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.76, Средняя потеря=0.0602]\n",
      "Эпоха 5: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.76, Средняя потеря=0.0604]\n",
      "Эпоха 6: 100%|██████████| 376/376 [13:12<00:00,  2.11s/batch, rmse=0.74, Средняя потеря=0.0586]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.74, Средняя потеря=0.0574]\n",
      "Эпоха 7: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.74, Средняя потеря=0.0578]\n",
      "Тестирование 7: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.75, Средняя потеря=0.0587]\n",
      "Эпоха 8: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.73, Средняя потеря=0.0568]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:11<00:00,  2.04s/batch, rmse=0.75, Средняя потеря=0.0595]\n",
      "Эпоха 9: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.73, Средняя потеря=0.0558]\n",
      "Тестирование 9: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.74, Средняя потеря=0.0584]\n",
      "Эпоха 10: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.71, Средняя потеря=0.0541]\n",
      "Тестирование 10: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.76, Средняя потеря=0.06]\n",
      "Эпоха 11: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.7, Средняя потеря=0.0526]\n",
      "Тестирование 11: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.75, Средняя потеря=0.0601]\n",
      "Эпоха 12: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.7, Средняя потеря=0.0518]\n",
      "Тестирование 12: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.76, Средняя потеря=0.062]\n",
      "Эпоха 13: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.68, Средняя потеря=0.0493]\n",
      "Тестирование 13: 100%|██████████| 94/94 [01:34<00:00,  1.00s/batch, rmse=0.76, Средняя потеря=0.0608]\n",
      "Эпоха 14: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.67, Средняя потеря=0.0482]\n",
      "Тестирование 14: 100%|██████████| 94/94 [01:34<00:00,  1.00s/batch, rmse=0.79, Средняя потеря=0.0664]\n",
      "Эпоха 15: 100%|██████████| 376/376 [06:20<00:00,  1.01s/batch, rmse=0.66, Средняя потеря=0.0462]\n",
      "Тестирование 15: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.78, Средняя потеря=0.0649]\n",
      "Эпоха 16:   4%|▍         | 15/376 [00:15<06:04,  1.01s/batch]"
     ]
    }
   ],
   "source": [
    "model_tabular = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b201dce-1247-487c-91ff-789e9ae5353c",
   "metadata": {},
   "source": [
    "MMM_project/notebooks/Models_transformer_decision-level_text/TransformerModelWithAttention_6_0.74_checkpoint.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935262c8-91c5-4016-808d-ec6c65511be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [06:28<00:00,  1.03s/batch, rmse=0.84, Средняя потеря=0.0743]\n",
      "Тестирование 1: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.79, Средняя потеря=0.0646]\n",
      "Эпоха 2: 100%|██████████| 376/376 [06:25<00:00,  1.03s/batch, rmse=0.78, Средняя потеря=0.0636]\n",
      "Тестирование 2: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.75, Средняя потеря=0.0596]\n",
      "Эпоха 3: 100%|██████████| 376/376 [06:26<00:00,  1.03s/batch, rmse=0.77, Средняя потеря=0.0622]\n",
      "Тестирование 3: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.77, Средняя потеря=0.0614]\n",
      "Эпоха 4: 100%|██████████| 376/376 [06:26<00:00,  1.03s/batch, rmse=0.76, Средняя потеря=0.0604]\n",
      "Тестирование 4: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Эпоха 5: 100%|██████████| 376/376 [06:26<00:00,  1.03s/batch, rmse=0.75, Средняя потеря=0.0603]\n",
      "Тестирование 5: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.74, Средняя потеря=0.0579]\n",
      "Эпоха 6: 100%|██████████| 376/376 [06:25<00:00,  1.03s/batch, rmse=0.75, Средняя потеря=0.0585]\n",
      "Тестирование 6: 100%|██████████| 94/94 [01:35<00:00,  1.02s/batch, rmse=0.74, Средняя потеря=0.0585]\n",
      "Эпоха 7: 100%|██████████| 376/376 [06:50<00:00,  1.09s/batch, rmse=0.74, Средняя потеря=0.058]\n",
      "Тестирование 7: 100%|██████████| 94/94 [01:43<00:00,  1.10s/batch, rmse=0.74, Средняя потеря=0.0583]\n",
      "Эпоха 8: 100%|██████████| 376/376 [06:56<00:00,  1.11s/batch, rmse=0.73, Средняя потеря=0.0564]\n",
      "Тестирование 8: 100%|██████████| 94/94 [01:43<00:00,  1.10s/batch, rmse=0.77, Средняя потеря=0.0623]\n",
      "Эпоха 9: 100%|██████████| 376/376 [06:35<00:00,  1.05s/batch, rmse=0.72, Средняя потеря=0.0552]\n",
      "Тестирование 9: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.75, Средняя потеря=0.0595]\n",
      "Эпоха 10: 100%|██████████| 376/376 [06:22<00:00,  1.02s/batch, rmse=0.71, Средняя потеря=0.0542]\n",
      "Тестирование 10: 100%|██████████| 94/94 [01:34<00:00,  1.01s/batch, rmse=0.76, Средняя потеря=0.0603]\n",
      "Эпоха 11: 100%|██████████| 376/376 [06:22<00:00,  1.02s/batch, rmse=0.7, Средняя потеря=0.052]\n",
      "Тестирование 11: 100%|██████████| 94/94 [01:38<00:00,  1.04s/batch, rmse=0.75, Средняя потеря=0.06]\n",
      "Эпоха 12: 100%|██████████| 376/376 [06:28<00:00,  1.03s/batch, rmse=0.69, Средняя потеря=0.0505]\n",
      "Тестирование 12: 100%|██████████| 94/94 [01:36<00:00,  1.02s/batch, rmse=0.75, Средняя потеря=0.0593]\n",
      "Эпоха 13:  48%|████▊     | 181/376 [03:06<03:24,  1.05s/batch]"
     ]
    }
   ],
   "source": [
    "model_tabular = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd31acd0-3738-4dab-a673-b48685212baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [07:03<00:00,  1.13s/batch, rmse=0.84, Средняя потеря=0.0764]\n",
      "Тестирование 1: 100%|██████████| 94/94 [01:41<00:00,  1.08s/batch, rmse=0.76, Средняя потеря=0.061]\n",
      "Эпоха 2: 100%|██████████| 376/376 [06:46<00:00,  1.08s/batch, rmse=0.78, Средняя потеря=0.0647]\n",
      "Тестирование 2: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.75, Средняя потеря=0.0601]\n",
      "Эпоха 3: 100%|██████████| 376/376 [06:51<00:00,  1.09s/batch, rmse=0.77, Средняя потеря=0.0624]\n",
      "Тестирование 3: 100%|██████████| 94/94 [01:41<00:00,  1.08s/batch, rmse=0.79, Средняя потеря=0.0671]\n",
      "Эпоха 4: 100%|██████████| 376/376 [06:49<00:00,  1.09s/batch, rmse=0.76, Средняя потеря=0.0619]\n",
      "Тестирование 4: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.76, Средняя потеря=0.0616]\n",
      "Эпоха 5: 100%|██████████| 376/376 [06:42<00:00,  1.07s/batch, rmse=0.75, Средняя потеря=0.0597]\n",
      "Тестирование 5: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.75, Средняя потеря=0.0591]\n",
      "Эпоха 6: 100%|██████████| 376/376 [07:03<00:00,  1.13s/batch, rmse=0.74, Средняя потеря=0.0586]\n",
      "Тестирование 6: 100%|██████████| 94/94 [01:39<00:00,  1.06s/batch, rmse=0.75, Средняя потеря=0.0585]\n",
      "Эпоха 7: 100%|██████████| 376/376 [06:42<00:00,  1.07s/batch, rmse=0.74, Средняя потеря=0.0581]\n",
      "Тестирование 7: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.74, Средняя потеря=0.0588]\n",
      "Эпоха 8: 100%|██████████| 376/376 [06:41<00:00,  1.07s/batch, rmse=0.73, Средняя потеря=0.0565]\n",
      "Тестирование 8: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.75, Средняя потеря=0.0586]\n",
      "Эпоха 9: 100%|██████████| 376/376 [06:44<00:00,  1.08s/batch, rmse=0.72, Средняя потеря=0.0554]\n",
      "Тестирование 9: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.75, Средняя потеря=0.0601]\n",
      "Эпоха 10: 100%|██████████| 376/376 [06:44<00:00,  1.08s/batch, rmse=0.72, Средняя потеря=0.0541]\n",
      "Тестирование 10: 100%|██████████| 94/94 [01:40<00:00,  1.06s/batch, rmse=0.75, Средняя потеря=0.0591]\n",
      "Эпоха 11: 100%|██████████| 376/376 [06:43<00:00,  1.07s/batch, rmse=0.7, Средняя потеря=0.0522]\n",
      "Тестирование 11: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.76, Средняя потеря=0.0618]\n",
      "Эпоха 12: 100%|██████████| 376/376 [06:46<00:00,  1.08s/batch, rmse=0.69, Средняя потеря=0.0509]\n",
      "Тестирование 12: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.76, Средняя потеря=0.0605]\n",
      "Эпоха 13: 100%|██████████| 376/376 [07:28<00:00,  1.19s/batch, rmse=0.69, Средняя потеря=0.0503]\n",
      "Тестирование 13: 100%|██████████| 94/94 [01:40<00:00,  1.07s/batch, rmse=0.76, Средняя потеря=0.0611]\n",
      "Эпоха 14: 100%|██████████| 376/376 [10:57<00:00,  1.75s/batch, rmse=0.67, Средняя потеря=0.0478]\n",
      "Тестирование 14: 100%|██████████| 94/94 [03:26<00:00,  2.20s/batch, rmse=0.77, Средняя потеря=0.0634]\n",
      "Эпоха 15: 100%|██████████| 376/376 [13:56<00:00,  2.23s/batch, rmse=0.66, Средняя потеря=0.0461]\n",
      "Тестирование 15: 100%|██████████| 94/94 [03:28<00:00,  2.22s/batch, rmse=0.77, Средняя потеря=0.0625]\n",
      "Эпоха 16: 100%|██████████| 376/376 [13:49<00:00,  2.21s/batch, rmse=0.65, Средняя потеря=0.0448]\n",
      "Тестирование 16: 100%|██████████| 94/94 [03:30<00:00,  2.24s/batch, rmse=0.8, Средняя потеря=0.0674]\n",
      "Эпоха 17: 100%|██████████| 376/376 [13:48<00:00,  2.20s/batch, rmse=0.63, Средняя потеря=0.0428]\n",
      "Тестирование 17: 100%|██████████| 94/94 [03:26<00:00,  2.20s/batch, rmse=0.79, Средняя потеря=0.067]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 17 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_tabular = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217c8418-561b-47df-8a63-9316e9d0c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9f488-9011-44cb-896d-8ece72b9b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [31:02<00:00,  4.95s/batch, rmse=0.92, Средняя потеря=0.0894]\n",
      "Тестирование 1: 100%|██████████| 94/94 [07:32<00:00,  4.81s/batch, rmse=0.86, Средняя потеря=0.0769]\n",
      "Эпоха 2: 100%|██████████| 376/376 [28:22<00:00,  4.53s/batch, rmse=0.88, Средняя потеря=0.0803]\n",
      "Тестирование 2: 100%|██████████| 94/94 [06:49<00:00,  4.36s/batch, rmse=0.86, Средняя потеря=0.0773]\n",
      "Эпоха 3: 100%|██████████| 376/376 [26:51<00:00,  4.29s/batch, rmse=0.85, Средняя потеря=0.0754]\n",
      "Тестирование 3: 100%|██████████| 94/94 [06:33<00:00,  4.19s/batch, rmse=0.86, Средняя потеря=0.0766]\n",
      "Эпоха 4: 100%|██████████| 376/376 [27:05<00:00,  4.32s/batch, rmse=0.84, Средняя потеря=0.0737]\n",
      "Тестирование 4: 100%|██████████| 94/94 [06:49<00:00,  4.36s/batch, rmse=0.85, Средняя потеря=0.077]\n",
      "Эпоха 5: 100%|██████████| 376/376 [27:33<00:00,  4.40s/batch, rmse=0.84, Средняя потеря=0.073]\n",
      "Тестирование 5: 100%|██████████| 94/94 [06:53<00:00,  4.39s/batch, rmse=0.84, Средняя потеря=0.0733]\n",
      "Эпоха 6: 100%|██████████| 376/376 [27:56<00:00,  4.46s/batch, rmse=0.83, Средняя потеря=0.0713]\n",
      "Тестирование 6: 100%|██████████| 94/94 [07:00<00:00,  4.48s/batch, rmse=0.83, Средняя потеря=0.0731]\n",
      "Эпоха 7: 100%|██████████| 376/376 [27:51<00:00,  4.45s/batch, rmse=0.82, Средняя потеря=0.0697]\n",
      "Тестирование 7: 100%|██████████| 94/94 [06:48<00:00,  4.35s/batch, rmse=0.85, Средняя потеря=0.0768]\n",
      "Эпоха 8: 100%|██████████| 376/376 [27:28<00:00,  4.38s/batch, rmse=0.8, Средняя потеря=0.0675]\n",
      "Тестирование 8: 100%|██████████| 94/94 [06:49<00:00,  4.36s/batch, rmse=0.86, Средняя потеря=0.078]\n",
      "Эпоха 9:  27%|██▋       | 103/376 [07:34<20:26,  4.49s/batch]"
     ]
    }
   ],
   "source": [
    "model_tabular = TransformerModelWithAttention(num_layers=2, input_dim=1024, hidden_dim=128, num_heads=2).to(device)\n",
    "optimizer = optim.Adam(params = model_tabular.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_tabular, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL + \"_\" + step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa48da1-4faa-4446-92e0-f631901ad580",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc40499-c3ce-4b12-b59c-015dede2f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 1024, hidden_size = 64, num_layers = 2, dropout = 0.1, bidirectional=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.lstm.bidirectional:\n",
    "            h0, c0 = torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(2 * self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        else:\n",
    "            h0, c0 = torch.zeros(self.num_layers, len(x), self.hidden_size).to(device), torch.zeros(self.num_layers, len(x), self.hidden_size).to(device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        if self.lstm.bidirectional:\n",
    "            out = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8a3579-8088-4bee-b9d4-21c12691dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=Dataset_avito('train', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=Dataset_avito('val', len_1=7517, len_2=7517), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec9df56-9842-4551-ac54-a9082950366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = os.path.join(ROOT_DIR, \"Models_lstm_decision-level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4126e1ee-f937-4a70-891e-ecadef3d72e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [13:40<00:00,  2.18s/batch, rmse=0.92, Средняя потеря=0.0879]\n",
      "Тестирование 1: 100%|██████████| 94/94 [03:25<00:00,  2.18s/batch, rmse=0.9, Средняя потеря=0.0844]\n",
      "Эпоха 2: 100%|██████████| 376/376 [14:10<00:00,  2.26s/batch, rmse=0.9, Средняя потеря=0.0854]\n",
      "Тестирование 2: 100%|██████████| 94/94 [03:26<00:00,  2.20s/batch, rmse=0.89, Средняя потеря=0.0836]\n",
      "Эпоха 3: 100%|██████████| 376/376 [13:58<00:00,  2.23s/batch, rmse=0.9, Средняя потеря=0.0853]\n",
      "Тестирование 3: 100%|██████████| 94/94 [03:25<00:00,  2.18s/batch, rmse=0.89, Средняя потеря=0.0832]\n",
      "Эпоха 4: 100%|██████████| 376/376 [14:00<00:00,  2.23s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 4: 100%|██████████| 94/94 [03:25<00:00,  2.19s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 5: 100%|██████████| 376/376 [13:58<00:00,  2.23s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 5: 100%|██████████| 94/94 [03:22<00:00,  2.15s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 6: 100%|██████████| 376/376 [13:55<00:00,  2.22s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 6: 100%|██████████| 94/94 [03:22<00:00,  2.15s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 7: 100%|██████████| 376/376 [13:38<00:00,  2.18s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 7: 100%|██████████| 94/94 [03:21<00:00,  2.15s/batch, rmse=0.89, Средняя потеря=0.0832]\n",
      "Эпоха 8: 100%|██████████| 376/376 [13:43<00:00,  2.19s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 8: 100%|██████████| 94/94 [03:18<00:00,  2.11s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 9: 100%|██████████| 376/376 [13:08<00:00,  2.10s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 9: 100%|██████████| 94/94 [03:10<00:00,  2.02s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 10: 100%|██████████| 376/376 [13:00<00:00,  2.08s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 10: 100%|██████████| 94/94 [03:14<00:00,  2.07s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 11: 100%|██████████| 376/376 [13:13<00:00,  2.11s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 11: 100%|██████████| 94/94 [03:16<00:00,  2.09s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 12: 100%|██████████| 376/376 [13:14<00:00,  2.11s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 12: 100%|██████████| 94/94 [03:17<00:00,  2.10s/batch, rmse=0.89, Средняя потеря=0.0832]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ранняя остановка на эпохе 12 из-за отсутствия улучшения точности на тестовой выборке\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm = LSTM().to(device)\n",
    "optimizer = optim.Adam(params = model_lstm.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_lstm, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e0ff9-5609-409e-a1ef-88476a7440dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 100%|██████████| 376/376 [11:40<00:00,  1.86s/batch, rmse=0.91, Средняя потеря=0.0875]\n",
      "Тестирование 1: 100%|██████████| 94/94 [02:53<00:00,  1.85s/batch, rmse=0.89, Средняя потеря=0.0838]\n",
      "Эпоха 2: 100%|██████████| 376/376 [11:45<00:00,  1.88s/batch, rmse=0.9, Средняя потеря=0.0854]\n",
      "Тестирование 2: 100%|██████████| 94/94 [02:54<00:00,  1.86s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 3: 100%|██████████| 376/376 [11:42<00:00,  1.87s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 3: 100%|██████████| 94/94 [02:59<00:00,  1.91s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 4: 100%|██████████| 376/376 [11:39<00:00,  1.86s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 4: 100%|██████████| 94/94 [02:57<00:00,  1.89s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 5: 100%|██████████| 376/376 [11:54<00:00,  1.90s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 5: 100%|██████████| 94/94 [02:56<00:00,  1.88s/batch, rmse=0.89, Средняя потеря=0.0833]\n",
      "Эпоха 6: 100%|██████████| 376/376 [11:57<00:00,  1.91s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 6: 100%|██████████| 94/94 [02:57<00:00,  1.88s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 7: 100%|██████████| 376/376 [11:54<00:00,  1.90s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 7: 100%|██████████| 94/94 [02:58<00:00,  1.89s/batch, rmse=0.88, Средняя потеря=0.0831]\n",
      "Эпоха 8: 100%|██████████| 376/376 [11:47<00:00,  1.88s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 8: 100%|██████████| 94/94 [02:56<00:00,  1.88s/batch, rmse=0.89, Средняя потеря=0.0831]\n",
      "Эпоха 9: 100%|██████████| 376/376 [11:51<00:00,  1.89s/batch, rmse=0.9, Средняя потеря=0.0852]\n",
      "Тестирование 9: 100%|██████████| 94/94 [02:55<00:00,  1.87s/batch, rmse=0.89, Средняя потеря=0.083]\n",
      "Эпоха 10: 100%|██████████| 376/376 [11:48<00:00,  1.88s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 10: 100%|██████████| 94/94 [02:54<00:00,  1.86s/batch, rmse=0.89, Средняя потеря=0.0834]\n",
      "Эпоха 11: 100%|██████████| 376/376 [11:41<00:00,  1.87s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 11: 100%|██████████| 94/94 [02:59<00:00,  1.90s/batch, rmse=0.89, Средняя потеря=0.083]\n",
      "Эпоха 12: 100%|██████████| 376/376 [11:35<00:00,  1.85s/batch, rmse=0.9, Средняя потеря=0.0851]\n",
      "Тестирование 12: 100%|██████████| 94/94 [02:51<00:00,  1.83s/batch, rmse=0.89, Средняя потеря=0.083]\n",
      "Эпоха 13:  97%|█████████▋| 366/376 [11:04<00:22,  2.21s/batch]"
     ]
    }
   ],
   "source": [
    "model_lstm = LSTM().to(device)\n",
    "optimizer = optim.Adam(params = model_lstm.parameters(), lr = LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "step = \"text\"\n",
    "trainer = ModelTrainer(model_lstm, train_dataloader, val_dataloader, device, EPOCHS, ROUND_LOSS, ROUND_RMSE, optimizer, loss_fn, step)\n",
    "trainer.train(PATH_TO_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
